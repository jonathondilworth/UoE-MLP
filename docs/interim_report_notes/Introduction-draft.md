### Introduction (Draft)

During the last six years there has been an increase in popularity of connectionist based approaches (specifically, variations on deep neural network architectures) to solving vision based pattern recognition problems [INSERT REFERENCE HERE]. This surge in popularity has been the result of numerous advancements, including an increase in the amount of available training data and compute power [INSERT REFERENCE HERE], as is demonstrated in [INSERT AlexNet, VGGNet, LeNet references here]. Although deep neural networks work well with large amounts of training data [INSERT REFERENCE HERE], the performance of these models typically drops off in situations where only small amounts of training data is available [INSERT REFERENCE TO LEARNING CURVE HERE]. This poses a problem to smaller businesses and organisations who may not have the appropriate amount of data to utilise these emergent technologies. Such a problem motivates the investigation presented within this (and the forthcoming) report that examines the application of fruitful techniques (specifically, transfer learning) to boost the performance of deep neural network architectures using small datasets.

In order to formalise the aforementioned problem, this paper investigates the effects of dramatically reducing the training set (§x.x) size and observes the effects on the accuracy of the proposed deep neural network architecture (§x.x). In addition, this procedure is applied to two comparable datasets, one with subtle differences between classes and the other with apparent differences between classes. These datasets are compared in order to gauge a fuller understanding of how the data itself can also affect the performance of deep neural networks.

The remainder of this paper 

OR..

An investigation into viable techniques to be employed in order to boost performance of smaller datasets within the domain of neural architectures is the primary motivation behind the research presented within this (and the forthcoming) report.

*alternative approach:*

During recent years there has been an influx in the amount of published researched in the field of connectionist approaches to solving pattern recognition problems. This popularity surge following the AlexNet Karpathy et al. paper has been the result of an increase in the amount of available data (big data) and compute (thanks to GPU developments). Although these connectionist models (neural networks) work well with large training datasets, the state of the art levels of accuracy are typically found to be achieved by models with thousands to tens of thousands of parameters. As the performance of a model can be defined in terms of its training set size (learning curves), we can see that this may be problematic for addressing scenarios where only small amounts of data are available.

**TO READ RIGHT NOW**

https://www.kdnuggets.com/2015/03/more-training-data-or-complex-models.html

**References**

These have actually been pretty difficult to find! A lot of these are still 'to read':

https://arxiv.org/pdf/1211.1323.pdf <- read this for sure

https://stats.stackexchange.com/questions/226672/how-few-training-examples-is-too-few-when-training-a-neural-network <- this is a good reference

https://www.quora.com/What-is-the-recommended-minimum-training-data-set-size-to-train-a-deep-neural-network

https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab

https://www.researchgate.net/post/What_is_the_minimum_sample_size_required_to_train_a_Deep_Learning_model-CNN

https://arxiv.org/pdf/1511.06348.pdf

http://sci2s.ugr.es/keel/pdf/specific/articulo/raudys91.pdf

http://carlvondrick.com/bigdata.pdf

https://medium.com/@malay.haldar/how-much-training-data-do-you-need-da8ec091e956

https://stats.stackexchange.com/questions/51490/how-large-a-training-set-is-needed

**Interesting**

http://people.idsia.ch/~ciresan/data/ijcnn2012_v9.pdf

LEARNING CURVE: https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472-6947-12-8


