{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, ELU, Activation, Lambda, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.initializers import glorot_uniform\n",
    "from random import randrange\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#from common import load_data, load_data_hog, train_and_save_results, L2Penalty\n",
    "\n",
    "seed=10102016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_1(learning):\n",
    "        \n",
    "    image_shape = (28, 28, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x, input_shape=image_shape, output_shape=image_shape))\n",
    "    \n",
    "    model.add(Convolution2D(\n",
    "        5, \n",
    "        5, \n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        border_mode=\"valid\"))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(5, kernel_initializer=glorot_uniform(seed), activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(lr=learning)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_2(learning):\n",
    "        \n",
    "    image_shape = (28, 28, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x, input_shape=image_shape, output_shape=image_shape))\n",
    "    \n",
    "    model.add(Convolution2D(\n",
    "        5, \n",
    "        5, \n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        border_mode=\"valid\"))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(\n",
    "        10, \n",
    "        5, \n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        border_mode=\"valid\"))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(5, kernel_initializer=glorot_uniform(seed), activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(lr=learning)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, 5, kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\", padding=\"valid\")`\n",
      "  del sys.path[0]\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, 5, kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\", padding=\"valid\")`\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, 5, kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\", padding=\"valid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 5)         130       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 5)         0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 12, 5)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 720)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 3605      \n",
      "=================================================================\n",
      "Total params: 3,735\n",
      "Trainable params: 3,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model 2 summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_2 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 5)         130       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 5)         0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 10)          1260      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 805       \n",
      "=================================================================\n",
      "Total params: 2,195\n",
      "Trainable params: 2,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "model1 = get_model_1(learning_rate)\n",
    "model2 = get_model_2(learning_rate)\n",
    "\n",
    "print(\"Model 1 summary\")\n",
    "model1.summary()\n",
    "\n",
    "print(\"Model 2 summary\")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_train, file_test, size=1.0):\n",
    "    data_train = np.load(file_train)\n",
    "    data_test = np.load(file_test)\n",
    "    \n",
    "    xtr, ytr = data_train[\"inputs\"], data_train[\"targets\"].reshape((-1, 1))\n",
    "    xte, yte = data_test[\"inputs\"], data_test[\"targets\"].reshape((-1, 1))\n",
    "\n",
    "    # use 15% of train data for testing\n",
    "    xtr, xva, ytr, yva = train_test_split(xtr, ytr, test_size=0.15, random_state=50)\n",
    "\n",
    "    if size >= 1.0 and size <=0:\n",
    "        return xtr, ytr, xva, yva, xte, yte\n",
    "    \n",
    "    discard_size = 1.0 - size\n",
    "    xtr, _, ytr, _ = train_test_split(xtr, ytr, test_size=discard_size, random_state=50)\n",
    "    xva, _, yva, _ = train_test_split(xva, yva, test_size=discard_size, random_state=50)\n",
    "    xte, _, yte, _ = train_test_split(xte, yte, test_size=discard_size, random_state=50)\n",
    "    \n",
    "    xtr, ytr = reshape_dataset(xtr, ytr.ravel())\n",
    "    xva, yva = reshape_dataset(xva, yva.ravel())\n",
    "    xte, yte = reshape_dataset(xte, yte.ravel())\n",
    "    \n",
    "#     print(xtr.shape)\n",
    "#     print(ytr.shape)\n",
    "    \n",
    "#     print(xva.shape)\n",
    "#     print(yva.shape)\n",
    "    \n",
    "#     print(xte.shape)\n",
    "#     print(yte.shape)\n",
    "    \n",
    "    return xtr, ytr, xva, yva, xte, yte\n",
    "\n",
    "def reshape_dataset(x, y):\n",
    "    n_elem, n_feat = x.shape\n",
    "    n_feat = int(n_feat**0.5)\n",
    "    x = x.reshape((n_elem, n_feat, n_feat, 1))\n",
    "\n",
    "    # make sure classes are between 0 and num_classes\n",
    "    new_y = np.ones_like(y)*(-1)\n",
    "    num_classes = np.unique(y)\n",
    "    for idx, label in enumerate(num_classes):\n",
    "        new_y[y == label] = idx\n",
    "    \n",
    "    y = to_categorical(new_y)\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "def load_clothes(size=1.0):\n",
    "    return load_dataset(\"../data/clothes_train.npz\", \"../data/clothes_test.npz\", size)\n",
    "    \n",
    "    \n",
    "def load_faces(size=1.0):\n",
    "    return load_dataset(\"../data/faces_train.npz\", \"../data/faces_test.npz\", size)\n",
    "\n",
    "# load_faces()\n",
    "# load_clothes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot_metrics(log_file_name, history):\n",
    "    keys = history.history.keys()\n",
    "\n",
    "    f, ax = plt.subplots(len(keys), 1, figsize=(5, 22))\n",
    "\n",
    "    for idx, k in enumerate(keys):\n",
    "        ax[idx].plot(history.history[k])\n",
    "        ax[idx].set_title(\"model \" + k)\n",
    "        ax[idx].set_ylabel(k)\n",
    "        ax[idx].set_xlabel('epoch')\n",
    "    \n",
    "    f.savefig(\"{:s}.png\".format(log_file_name), dpi=90)\n",
    "\n",
    "def save_log_metrics(log_file_name, hyper, history):\n",
    "    header = \"\"\n",
    "\n",
    "    for key in hyper:\n",
    "        header = header + \", \" + key + \": \" + str(hyper[key])\n",
    "\n",
    "    header = header[2:]\n",
    "\n",
    "    with open(log_file_name + \".txt\", \"w+\") as log_file:\n",
    "        log_file.write(header+\"\\n\")\n",
    "        \n",
    "        keys = history.history.keys()\n",
    "        head = \"\"\n",
    "        \n",
    "        c = 0\n",
    "        for k in keys:\n",
    "            if c == 0:\n",
    "                l = len(history.history[k]) # number of epochs\n",
    "                h = np.zeros(l)\n",
    "            head = head + k + \",\"\n",
    "            h = np.vstack((h, history.history[k]))\n",
    "            c = c + 1\n",
    "\n",
    "        head = head[:-1]\n",
    "        head = head + \"\\n\"\n",
    "        log_file.write(head)\n",
    "\n",
    "        h = h[1:,:]\n",
    "        h = h.T\n",
    "\n",
    "        for row in h:\n",
    "            new_line = \"\"\n",
    "            for value in row:\n",
    "                new_line = new_line + \"{:.8f},\".format(value)\n",
    "            new_line = new_line[:-1]\n",
    "            new_line = new_line + \"\\n\"\n",
    "            log_file.write(new_line)\n",
    "\n",
    "    log_file.close()\n",
    "\n",
    "def generator(X, y, batch_size):\n",
    "    total_input = len(X)\n",
    "    \n",
    "    while True:\n",
    "        features, targets = [], []\n",
    "        i = 0\n",
    "        while len(features) < batch_size:\n",
    "            index = randrange(0, total_input)\n",
    "            feats = X[index]\n",
    "            labels = y[index]\n",
    "           \n",
    "            features.append(feats)\n",
    "            targets.append(labels)\n",
    "            \n",
    "        yield (np.array(features), np.array(targets))\n",
    "\n",
    "def getFeaturesTargets(X, y):\n",
    "    feats = []\n",
    "    targets = []\n",
    "\n",
    "    for feat, label in zip(X, y):\n",
    "        feats.append(feat)\n",
    "        targets.append(label)\n",
    "\n",
    "    return np.array(feats), np.array(targets)\n",
    "\n",
    "# arg dataset is 0 for clothes, 1 for faces\n",
    "def train_model(model, hyper_params, log_file_name, dataset=0, dataset_size=1.0):\n",
    "    learning_rate = hyper_params[\"learning_rate\"]\n",
    "    training_size = hyper_params[\"training_size\"]\n",
    "    batch_size = hyper_params[\"batch_size\"]\n",
    "    num_epochs = hyper_params[\"num_epochs\"]\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    #train_data, valid_data, test_data = load_data(rng, batch_size=hyper_params[\"batch_size\"])\n",
    "    if dataset == 0:\n",
    "        xtr, ytr, xva, yva, xte, yte = load_clothes(dataset_size)\n",
    "    else:\n",
    "        xtr, ytr, xva, yva, xte, yte = load_faces(dataset_size)\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        generator(xtr, ytr, batch_size),\n",
    "        samples_per_epoch = training_size,\n",
    "        validation_data = getFeaturesTargets(xva, yva),\n",
    "        nb_epoch = num_epochs\n",
    "        )\n",
    "\n",
    "    eval_ = model.evaluate(xte, yte)\n",
    "    for val, key in zip(eval_, model.metrics_names):\n",
    "        hyper_params[key] = val\n",
    "\n",
    "    save_log_metrics(log_file_name, hyper_params, history)\n",
    "    save_plot_metrics(log_file_name, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, 5, kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\", padding=\"valid\")`\n",
      "  del sys.path[0]\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, 5, kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\", padding=\"valid\")`\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, 5, kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\", padding=\"valid\")`\n"
     ]
    }
   ],
   "source": [
    "hyper = OrderedDict()\n",
    "hyper[\"learning_rate\"] = 0.01\n",
    "hyper[\"training_size\"] = 2000\n",
    "hyper[\"batch_size\"] = 50\n",
    "hyper[\"num_epochs\"] = 10\n",
    "\n",
    "model1 = get_model_1(hyper[\"learning_rate\"])\n",
    "model2 = get_model_2(hyper[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1 with clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  19/2000 [..............................] - ETA: 18s - loss: 0.1511 - acc: 0.9495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=(array([[[..., steps_per_epoch=2000, epochs=10)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 19s - loss: 0.1712 - acc: 0.9440 - val_loss: 0.2643 - val_acc: 0.9296\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 18s - loss: 0.1721 - acc: 0.9440 - val_loss: 0.2209 - val_acc: 0.9307\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 19s - loss: 0.1666 - acc: 0.9459 - val_loss: 0.2003 - val_acc: 0.9418\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 17s - loss: 0.1721 - acc: 0.9441 - val_loss: 0.2077 - val_acc: 0.9342\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 17s - loss: 0.1695 - acc: 0.9446 - val_loss: 0.2163 - val_acc: 0.9376\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 17s - loss: 0.1710 - acc: 0.9441 - val_loss: 0.2159 - val_acc: 0.9387\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 18s - loss: 0.1612 - acc: 0.9476 - val_loss: 0.2441 - val_acc: 0.9189\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 22s - loss: 0.1741 - acc: 0.9432 - val_loss: 0.1979 - val_acc: 0.9382\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 19s - loss: 0.1603 - acc: 0.9470 - val_loss: 0.2130 - val_acc: 0.9242\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 18s - loss: 0.1814 - acc: 0.9390 - val_loss: 0.1915 - val_acc: 0.9382\n",
      "4928/5000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "train_model(model1, hyper, \"model1_clothes\", dataset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1 with faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  19/2000 [..............................] - ETA: 19s - loss: 2.8665 - acc: 0.0821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=(array([[[..., steps_per_epoch=2000, epochs=10)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 17s - loss: 1.4034 - acc: 0.3872 - val_loss: 1.3603 - val_acc: 0.3853\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 18s - loss: 1.3625 - acc: 0.3885 - val_loss: 1.3589 - val_acc: 0.3858\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 18s - loss: 1.3607 - acc: 0.3910 - val_loss: 1.3608 - val_acc: 0.3861\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 17s - loss: 1.3599 - acc: 0.3915 - val_loss: 1.3626 - val_acc: 0.3869\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 16s - loss: 1.3543 - acc: 0.3934 - val_loss: 1.3743 - val_acc: 0.3855\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 17s - loss: 1.3551 - acc: 0.3918 - val_loss: 1.3674 - val_acc: 0.3861\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 21s - loss: 1.3568 - acc: 0.3940 - val_loss: 1.3709 - val_acc: 0.3858\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 19s - loss: 1.3565 - acc: 0.3915 - val_loss: 1.3717 - val_acc: 0.3872\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 21s - loss: 1.3573 - acc: 0.3923 - val_loss: 1.3752 - val_acc: 0.3861\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 19s - loss: 1.3569 - acc: 0.3921 - val_loss: 1.3808 - val_acc: 0.3861\n",
      "3680/3964 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "train_model(model1, hyper, \"model1_faces\", dataset=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2 with clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=(array([[[..., steps_per_epoch=2000, epochs=10)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 22s - loss: 0.1915 - acc: 0.9418 - val_loss: 0.1802 - val_acc: 0.9436\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 25s - loss: 0.1640 - acc: 0.9509 - val_loss: 0.1656 - val_acc: 0.9429\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 26s - loss: 0.1738 - acc: 0.9487 - val_loss: 0.1584 - val_acc: 0.9431\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 25s - loss: 0.1681 - acc: 0.9485 - val_loss: 0.2167 - val_acc: 0.9169\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 24s - loss: 0.1732 - acc: 0.9480 - val_loss: 0.2195 - val_acc: 0.9327\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 23s - loss: 0.1711 - acc: 0.9480 - val_loss: 0.1689 - val_acc: 0.9413\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 25s - loss: 0.1716 - acc: 0.9456 - val_loss: 0.2208 - val_acc: 0.9429\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 23s - loss: 0.1756 - acc: 0.9494 - val_loss: 0.1868 - val_acc: 0.9433\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 23s - loss: 0.1795 - acc: 0.9476 - val_loss: 0.2728 - val_acc: 0.9418\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 24s - loss: 0.1794 - acc: 0.9480 - val_loss: 0.2053 - val_acc: 0.9447\n",
      "4640/5000 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "train_model(model2, hyper, \"model2_clothes\", dataset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2 with faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  16/2000 [..............................] - ETA: 23s - loss: 5.4483 - acc: 0.3100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "/afs/inf.ed.ac.uk/user/s17/s1700260/miniconda3/envs/mlp/lib/python3.6/site-packages/ipykernel_launcher.py:99: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=(array([[[..., steps_per_epoch=2000, epochs=10)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 23s - loss: 1.4226 - acc: 0.3778 - val_loss: 1.3649 - val_acc: 0.3889\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 20s - loss: 1.3630 - acc: 0.3880 - val_loss: 1.3749 - val_acc: 0.3889\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 19s - loss: 1.3652 - acc: 0.3884 - val_loss: 1.3712 - val_acc: 0.3906\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 21s - loss: 1.3614 - acc: 0.3916 - val_loss: 1.3758 - val_acc: 0.3881\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 24s - loss: 1.3629 - acc: 0.3912 - val_loss: 1.3717 - val_acc: 0.3892\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 23s - loss: 1.3573 - acc: 0.3926 - val_loss: 1.3648 - val_acc: 0.3886\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 23s - loss: 1.3582 - acc: 0.3916 - val_loss: 1.3841 - val_acc: 0.3883\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 21s - loss: 1.3570 - acc: 0.3951 - val_loss: 1.3759 - val_acc: 0.3872\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 23s - loss: 1.3519 - acc: 0.3970 - val_loss: 1.3750 - val_acc: 0.3883\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 23s - loss: 1.3525 - acc: 0.3942 - val_loss: 1.3877 - val_acc: 0.3881\n",
      "3872/3964 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "train_model(model2, hyper, \"model2_faces\", dataset=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
