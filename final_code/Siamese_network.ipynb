{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mlp/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import scipy.misc\n",
    "import os\n",
    "import numpy.random as rng\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import scipy\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4096)         23107392    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,111,489\n",
      "Trainable params: 8,396,801\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##Some part of the code has written with reference to https://github.com/ascourge21/Siamese\n",
    "##Our proposed VGG16 augmented siamese network:\n",
    "input_shape = (64,64,3)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "VGG=VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(64,64,3), pooling=None)\n",
    "model = Sequential()\n",
    "for i in range(19):\n",
    "    model.add(VGG.get_layer(index=i))\n",
    "model.add(Flatten(input_shape=VGG.output_shape[1:]))\n",
    "model.add(Dense(4096, activation='sigmoid',kernel_regularizer=l2(0.0001)))\n",
    "first_twin = model(left_input)\n",
    "second_twin = model(right_input)\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[19:21]:\n",
    "    layer.trainable = True\n",
    "L1_layer =Lambda(lambda x: K.abs(x[0]-x[1]))\n",
    "L1_distance = L1_layer([first_twin, second_twin])\n",
    "prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "Siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "#optimizer=optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False )\n",
    "optimizer = keras.optimizers.Adam(0.0001)\n",
    "Siamese_net.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "Siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_24\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "19 flatten_8\n",
      "20 dense_15\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class0=os.path.join(os.getcwd() +'/clothes/train_rgb/0/' )\n",
    "class1=os.path.join(os.getcwd() +'/clothes/train_rgb/1/' )\n",
    "class2=os.path.join(os.getcwd() +'/clothes/train_rgb/2/' )\n",
    "class3=os.path.join(os.getcwd() +'/clothes/train_rgb/3/' )\n",
    "class4=os.path.join(os.getcwd() +'/clothes/train_rgb/4/' )\n",
    "class5=os.path.join(os.getcwd() +'/clothes/train_rgb/5/' )\n",
    "class6=os.path.join(os.getcwd() +'/clothes/train_rgb/6/' )\n",
    "Class0=os.path.join(os.getcwd() +'/face/valid_rgb/0/' )\n",
    "Class1=os.path.join(os.getcwd() +'/face/valid_rgb/1/' )\n",
    "Class2=os.path.join(os.getcwd() +'/face/valid_rgb/2/' )\n",
    "Class3=os.path.join(os.getcwd() +'/face/valid_rgb/3/' )\n",
    "Class4=os.path.join(os.getcwd() +'/face/valid_rgb/4/' )\n",
    "Class5=os.path.join(os.getcwd() +'/face/valid_rgb/5/' )\n",
    "Class6=os.path.join(os.getcwd() +'/face/valid_rgb/6/' )\n",
    "_0=os.path.join(os.getcwd() +'/face/test_split_rgb/0/' )\n",
    "_1=os.path.join(os.getcwd() +'/face/test_split_rgb/1/' )\n",
    "_2=os.path.join(os.getcwd() +'/face/test_split_rgb/2/' )\n",
    "_3=os.path.join(os.getcwd() +'/face/test_split_rgb/3/' )\n",
    "_4=os.path.join(os.getcwd() +'/face/test_split_rgb/4/' )\n",
    "_5=os.path.join(os.getcwd() +'/face/test_split_rgb/5/' )\n",
    "_6=os.path.join(os.getcwd() +'/face/test_split_rgb/6/' )\n",
    "AN=os.path.join(os.getcwd() +'/Jpface/AN/' )\n",
    "FE=os.path.join(os.getcwd() +'/Jpface/FE/' )\n",
    "HA=os.path.join(os.getcwd() +'/Jpface/HA/' )\n",
    "NE=os.path.join(os.getcwd() +'/Jpface/NE/' )\n",
    "SA=os.path.join(os.getcwd() +'/Jpface/SA/' )\n",
    "SU=os.path.join(os.getcwd() +'/Jpface/SU/' )\n",
    "DI=os.path.join(os.getcwd() +'/Jpface/DI/' )\n",
    "#50% same 50% different class ,randomly sampled pairs.\n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = scipy.misc.imread(os.path.join(folder,filename))\n",
    "        images.append(img)\n",
    "    return images\n",
    "## Different list represents two approaches for training: 3 classes of animated facial expression dataset and \n",
    "## 7 classes of Japanese female dataset\n",
    "#Train=[load_images(AN),load_images(DI),load_images(FE),load_images(HA),load_images(NE),load_images(SA),load_images(SU)]\n",
    "#train_image=[load_images(Class0),load_images(Class1),load_images(Class2)]\n",
    "#test_image=[load_images(_0),load_images(_1),load_images(_2),load_images(_3),load_images(_4),load_images(_5),load_images(_6)]\n",
    "#train_acc_image=[load_images(_0),load_images(_1),load_images(_2)]\n",
    "#Test_image=[load_images(_3),load_images(_4),load_images(_5),load_images(_6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"Create batch of 42 training pairs, 21 pairs with same class, 21 pairs with different class\"\"\"\n",
    "batch_size=36\n",
    "w, h = 64,64\n",
    "pairs=[np.zeros((batch_size, w, h,3)) for i in range(2)]\n",
    "#Remember to change setting when using face as train data:\n",
    "catagory_11=np.array([0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2]) #catagory when using 3 class of face as train data\n",
    "idx=np.random.choice(900,batch_size,replace=False)\n",
    "#catagory_11=np.array([0,0,0,0,0,0,0,1,1,1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,4,4,4,5,5,5,5,5,5,5,6,6,6,6,6,6,6])\n",
    "#catagory_11=np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2, \\\n",
    "#                      3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,\\\n",
    "#                      6,6,6,6,6,6,6,6,6,6,6,6,6,6])\n",
    "\"np.random.seed(10)\"\n",
    "\"idx=np.random.choice(6000,42,replace=False)\"\n",
    "\"Code commented are implemented separately to avoid fixed seed for random processes in future codes.\"\n",
    "\n",
    "#idxx=[1789, 5566, 1657, 3983, 5619, 1933,  449,  492, 1172, 2040, 1638,\n",
    "#2864,   43, 5540, 3758, 5606, 4936, 3399, 5436, 2602,  708, 4879,\n",
    "#5593, 4736, 1324,  952, 3036, 4971, 2630, 2543, 4627, 1044,  502,\n",
    "#1477, 5107, 4155, 2501, 3975,  143, 2083, 2514, 2569]\n",
    "#b=list(np.arange(0,6000))\n",
    "#for i in idxx:\n",
    "#    b.remove(i)\n",
    "\n",
    "\n",
    "\"Since we are only using 42 pairs of photos, we can treat remaning entire training images as validation set. There is now\"\n",
    "\"two separate dataset for training and validation.\"\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "# Note that catagory,pairs,targets,idx in the definitions below changes based on the approach used.\n",
    "def load_batch_train_pairs(batch_size):\n",
    "    pairs=[np.zeros((batch_size, 64, 64,3)) for i in range(2)]\n",
    "    targets=np.zeros((batch_size,))\n",
    "    targets[batch_size//2:] = 1\n",
    "    a=np.random.choice(900,6,replace=False)\n",
    "    idx=np.random.choice(900,36,replace=False)\n",
    "    #idx=np.hstack((a,a,a,a,a,a,a,a,a,a,a,a,a,a))\n",
    "    catagory_12=np.hstack((np.random.choice([1,2],size=(6,)), \\\n",
    "            np.random.choice([0,2],size=(6,)),\\\n",
    "            np.random.choice([0,1],size=(6,))))\n",
    "    #catagory_12=np.hstack((np.random.choice([1,2,3,4,5,6],size=(14,)), \\\n",
    "    #        np.random.choice([0,2,3,4,5,6],size=(14,)), \\\n",
    "    #        np.random.choice([0,1,3,4,5,6],size=(14,)), \\\n",
    "    #        np.random.choice([0,1,2,4,5,6],size=(14,)), \\\n",
    "    #        np.random.choice([0,1,2,3,5,6],size=(14,)), \\\n",
    "    #        np.random.choice([0,1,2,3,4,6],size=(14,)), \\\n",
    "    #        np.random.choice([0,1,2,3,4,5],size=(14,))))\n",
    "    for i in range(batch_size):\n",
    "        if i<batch_size//2:\n",
    "            pairs[0][i,:,:,:]=valid_image[catagory_11[i]][idx[i]]\n",
    "            pairs[1][i,:,:,:]=valid_image[catagory_12[i]][idx[i]]\n",
    "        if i>=batch_size//2:\n",
    "            pairs[0][i,:,:,:]=valid_image[catagory_11[i-(batch_size//2)]][idx[i]] \n",
    "            pairs[1][i,:,:,:]=valid_image[catagory_11[i-(batch_size//2)]][idx[i]]\n",
    "    return pairs,targets\n",
    "def get_one_shot(n_ex,N,image_set):\n",
    "    \"n_ex = total no. of examples\"\n",
    "    \"N = no.of class/N classes to distinguish from\"\n",
    "    categories = rng.choice(range(3),size=(N,),replace=False)            \n",
    "    true_category = categories[0]\n",
    "    index=rng.choice(n_ex,replace=False,size=(N,))\n",
    "    ex1, ex2 = rng.choice(n_ex,replace=False,size=(2,))\n",
    "    test_image_set = np.asarray([image_set[true_category][ex1]]*N)\n",
    "    support_set=np.zeros((N,64,64,3))\n",
    "    for i in range(1,N):\n",
    "        support_set[i,:,:,:] = image_set[categories[i]][index[i]]\n",
    "        support_set[0,:,:,:] = image_set[true_category][ex2]\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image_set, support_set = shuffle(targets, test_image_set, support_set) #Shuffle and give the same random orders of \n",
    "    #correct & incorrect pairs in three sets. \n",
    "    pairs = [test_image_set,support_set]\n",
    "    return pairs,targets\n",
    "def oneshot_accuracy(model,k,N,n_ex,image_set,verbose=0):\n",
    "    \"Calculation for accuracy of one-shot task to validation/test set after training the model\"\n",
    "    n_correct=0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ...\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs,targets=get_one_shot(n_ex,N,image_set)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% over {} random {} way one-shot learning accuracy\".format(percent_correct,k,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 22.8125% over 320 random 7 way one-shot learning accuracy\n",
      "22.8125\n"
     ]
    }
   ],
   "source": [
    "val_acc = oneshot_accuracy(Siamese_net,320,7,6000,test_image,verbose=True)\n",
    "print(val_acc)\n",
    "##Major results:\n",
    "##first approach:using first 3 class of facial expression as dataset.\n",
    "##For 20 epoch training\n",
    "#45%,47.185%,41.5625,44,375,40.625,49.0625,42.1875,49.375,41.6525,43.75 \n",
    "##Second Approach: Japanese female facial expression (7 classes 213 photos in total)\n",
    "##For 50epoch training on japanese face dataset(same image in the same class),100epoch,batch_size=98:\n",
    "##28.4375%,25.625%accuracy\n",
    "##Increasing training times seem do not improve accuracy furthermore ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter=50\n",
    "batch_size=36\n",
    "eval_step=5\n",
    "for z in range(1, n_iter):\n",
    "    (inputs,targets)=load_batch_train_pairs(batch_size)\n",
    "    loss=Siamese_net.train_on_batch(inputs,targets)\n",
    "    print(loss)\n",
    "    if n_iter%eval_step==0:\n",
    "        train_acc = oneshot_accuracy(Siamese_net,320,3,6000,train_acc_image,verbose=True)\n",
    "        print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Results from previous cell:\n",
    "1.26344\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 49.6875% over 320 random 3 way one-shot learning accuracy\n",
    "49.6875\n",
    "0.998732\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 56.5625% over 320 random 3 way one-shot learning accuracy\n",
    "56.5625\n",
    "0.920142\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 59.6875% over 320 random 3 way one-shot learning accuracy\n",
    "59.6875\n",
    "0.896159\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 62.8125% over 320 random 3 way one-shot learning accuracy\n",
    "62.8125\n",
    "0.886204\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 64.375% over 320 random 3 way one-shot learning accuracy\n",
    "64.375\n",
    "0.881523\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 64.375% over 320 random 3 way one-shot learning accuracy\n",
    "64.375\n",
    "0.881079\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 60.625% over 320 random 3 way one-shot learning accuracy\n",
    "60.625\n",
    "0.874748\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 65.625% over 320 random 3 way one-shot learning accuracy\n",
    "65.625\n",
    "0.870141\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 64.0625% over 320 random 3 way one-shot learning accuracy\n",
    "64.0625\n",
    "0.866885\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 68.4375% over 320 random 3 way one-shot learning accuracy\n",
    "68.4375\n",
    "0.863283\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ...\n",
    "Got an average of 63.4375% over 320 random 3 way one-shot learning accuracy\n",
    "63.4375\n",
    "0.860576\n",
    "Evaluating model on 320 random 3 way one-shot learning tasks ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 26.25% over 320 random 7 way one-shot learning accuracy\n",
      "26.25\n",
      "0.866949\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 27.1875% over 320 random 7 way one-shot learning accuracy\n",
      "27.1875\n",
      "0.861861\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 26.25% over 320 random 7 way one-shot learning accuracy\n",
      "26.25\n",
      "0.858603\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 21.25% over 320 random 7 way one-shot learning accuracy\n",
      "21.25\n",
      "0.852847\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 21.25% over 320 random 7 way one-shot learning accuracy\n",
      "21.25\n",
      "0.849759\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 27.5% over 320 random 7 way one-shot learning accuracy\n",
      "27.5\n",
      "0.845707\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 25.0% over 320 random 7 way one-shot learning accuracy\n",
      "25.0\n",
      "0.841537\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 27.8125% over 320 random 7 way one-shot learning accuracy\n",
      "27.8125\n",
      "0.838049\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 24.6875% over 320 random 7 way one-shot learning accuracy\n",
      "24.6875\n",
      "0.834458\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 28.75% over 320 random 7 way one-shot learning accuracy\n",
      "28.75\n",
      "0.830006\n",
      "Evaluating model on 320 random 7 way one-shot learning tasks ...\n",
      "Got an average of 24.0625% over 320 random 7 way one-shot learning accuracy\n",
      "24.0625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7387dc3b91b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_batch_train_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_acc = oneshot_accuracy(Siamese_net,320,7,6000,test_image,verbose=True)\n",
    "print(val_acc)\n",
    "\n",
    "n_iter=15\n",
    "batch_size=196\n",
    "eval_step=3\n",
    "for z in range(1, n_iter):\n",
    "    (inputs,targets)=load_batch_train_pairs(batch_size)\n",
    "    loss=Siamese_net.train_on_batch(inputs,targets)\n",
    "    print(loss)\n",
    "    if n_iter%eval_step==0:\n",
    "        val_acc = oneshot_accuracy(Siamese_net,320,7,6000,test_image,verbose=True)\n",
    "        print(val_acc)\n",
    "    #print(\"evaluating\")\n",
    "    #val_acc = oneshot_accuracy(siamese_net,100,7,900,valid_image,verbose=True)\n",
    "    #print(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822696\n",
      "0.81893\n",
      "0.815235\n",
      "0.811667\n",
      "0.808363\n",
      "0.804517\n",
      "0.801224\n",
      "0.797266\n",
      "0.794281\n",
      "0.790188\n",
      "0.786906\n",
      "0.7835\n",
      "0.780472\n",
      "0.776521\n",
      "0.774148\n",
      "0.769965\n",
      "0.766446\n",
      "0.763028\n",
      "0.759882\n",
      "0.756644\n",
      "0.753691\n",
      "0.750119\n",
      "0.746706\n",
      "0.743668\n"
     ]
    }
   ],
   "source": [
    "n_iter=25\n",
    "batch_size=196\n",
    "for z in range(1, n_iter):\n",
    "    (inputs,targets)=load_batch_train_pairs(batch_size)\n",
    "    loss=Siamese_net.train_on_batch(inputs,targets)\n",
    "    print(loss)\n",
    "    #print(\"evaluating\")\n",
    "    #val_acc = oneshot_accuracy(siamese_net,100,7,900,valid_image,verbose=True)\n",
    "    #print(val_acc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
